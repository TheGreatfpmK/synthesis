2024-01-16 22:02:00,581 - cli.py - This is Paynt version 0.1.0.
2024-01-16 22:02:00,581 - sketch.py - loading sketch from /home/fpmk/research/synthesis/experiments/../models/mdp/cav24/dodge-8-mod2-pull-30/sketch.templ ...
2024-01-16 22:02:00,581 - sketch.py - assuming sketch in PRISM format...
2024-01-16 22:02:00,589 - prism_parser.py - PRISM model type: MDP
2024-01-16 22:02:00,589 - prism_parser.py - processing hole definitions...
2024-01-16 22:02:00,589 - prism_parser.py - loading properties from /home/fpmk/research/synthesis/experiments/../models/mdp/cav24/dodge-8-mod2-pull-30/sketch.props ...
2024-01-16 22:02:00,590 - prism_parser.py - found the following specification: constraints: P>9/10 [F ((x = 8) & (y = 8))]; 
2024-01-16 22:02:00,590 - jani.py - constructing JANI program...
2024-01-16 22:02:00,593 - jani.py - constructing the quotient...
2024-01-16 22:02:01,724 - jani.py - associating choices of the quotient with hole assignments...
2024-01-16 22:02:02,175 - sketch.py - sketch parsing OK
2024-01-16 22:02:02,175 - sketch.py - constructed explicit quotient having 149659 states and 723570 actions
2024-01-16 22:02:02,175 - sketch.py - found the following specification constraints: P>9/10 [F ((x = 8) & (y = 8))]; 
2024-01-16 22:02:02,435 - mdp_family.py - associating choices with action labels...
2024-01-16 22:02:03,420 - synthesizer.py - evaluation initiated, design space: 30000
> progress 0.0%, elapsed 4 s, estimated 4546781 s (52 days), iters = {game: 2, MDP: 1}
> progress 0.0%, elapsed 8 s, estimated 8294746 s (96 days), iters = {game: 4, MDP: 3}
> progress 2.4%, elapsed 12 s, estimated 507 s, iters = {game: 7, MDP: 3}
> progress 4.0%, elapsed 15 s, estimated 388 s, iters = {game: 9, MDP: 4}
> progress 8.0%, elapsed 19 s, estimated 247 s, iters = {game: 12, MDP: 4}
> progress 8.8%, elapsed 22 s, estimated 260 s, iters = {game: 14, MDP: 5}
> progress 12.0%, elapsed 27 s, estimated 227 s, iters = {game: 17, MDP: 5}
> progress 13.6%, elapsed 30 s, estimated 223 s, iters = {game: 19, MDP: 6}
> progress 20.0%, elapsed 34 s, estimated 173 s, iters = {game: 22, MDP: 6}
> progress 20.0%, elapsed 38 s, estimated 191 s, iters = {game: 24, MDP: 8}
> progress 22.4%, elapsed 42 s, estimated 187 s, iters = {game: 27, MDP: 8}
> progress 24.0%, elapsed 45 s, estimated 188 s, iters = {game: 29, MDP: 9}
> progress 24.4%, elapsed 48 s, estimated 198 s, iters = {game: 32, MDP: 10}
> progress 26.0%, elapsed 51 s, estimated 198 s, iters = {game: 34, MDP: 11}
> progress 26.0%, elapsed 55 s, estimated 213 s, iters = {game: 36, MDP: 12}
> progress 26.15%, elapsed 58 s, estimated 224 s, iters = {game: 41, MDP: 14}
> progress 26.5%, elapsed 62 s, estimated 235 s, iters = {game: 45, MDP: 15}
> progress 26.75%, elapsed 65 s, estimated 246 s, iters = {game: 52, MDP: 17}
> progress 27.0%, elapsed 69 s, estimated 255 s, iters = {game: 58, MDP: 18}
> progress 27.0%, elapsed 72 s, estimated 267 s, iters = {game: 60, MDP: 20}
> progress 27.25%, elapsed 75 s, estimated 278 s, iters = {game: 67, MDP: 22}
> progress 27.5%, elapsed 79 s, estimated 288 s, iters = {game: 73, MDP: 23}
> progress 27.8%, elapsed 83 s, estimated 299 s, iters = {game: 77, MDP: 24}
> progress 28.0%, elapsed 87 s, estimated 313 s, iters = {game: 80, MDP: 25}
> progress 29.6%, elapsed 91 s, estimated 310 s, iters = {game: 83, MDP: 26}
> progress 32.0%, elapsed 96 s, estimated 300 s, iters = {game: 86, MDP: 26}
> progress 33.6%, elapsed 100 s, estimated 298 s, iters = {game: 89, MDP: 27}
> progress 33.92%, elapsed 103 s, estimated 305 s, iters = {game: 92, MDP: 28}
> progress 36.0%, elapsed 106 s, estimated 297 s, iters = {game: 94, MDP: 29}
> progress 37.6%, elapsed 110 s, estimated 294 s, iters = {game: 97, MDP: 29}
> progress 40.0%, elapsed 115 s, estimated 287 s, iters = {game: 100, MDP: 29}
> progress 40.0%, elapsed 119 s, estimated 298 s, iters = {game: 102, MDP: 31}
> progress 41.6%, elapsed 123 s, estimated 297 s, iters = {game: 105, MDP: 32}
> progress 44.0%, elapsed 126 s, estimated 288 s, iters = {game: 107, MDP: 33}
> progress 45.6%, elapsed 131 s, estimated 287 s, iters = {game: 110, MDP: 33}
> progress 48.0%, elapsed 134 s, estimated 280 s, iters = {game: 112, MDP: 34}
> progress 50.4%, elapsed 138 s, estimated 275 s, iters = {game: 115, MDP: 34}
> progress 52.0%, elapsed 142 s, estimated 273 s, iters = {game: 117, MDP: 35}
> progress 56.799%, elapsed 146 s, estimated 257 s, iters = {game: 120, MDP: 35}
> progress 60.0%, elapsed 149 s, estimated 249 s, iters = {game: 122, MDP: 36}
> progress 61.6%, elapsed 154 s, estimated 250 s, iters = {game: 125, MDP: 37}
> progress 64.0%, elapsed 157 s, estimated 246 s, iters = {game: 127, MDP: 38}
> progress 65.6%, elapsed 161 s, estimated 245 s, iters = {game: 130, MDP: 38}
> progress 66.08%, elapsed 164 s, estimated 249 s, iters = {game: 134, MDP: 40}
> progress 66.4%, elapsed 167 s, estimated 252 s, iters = {game: 138, MDP: 40}
> progress 68.0%, elapsed 171 s, estimated 251 s, iters = {game: 140, MDP: 41}
> progress 72.0%, elapsed 175 s, estimated 243 s, iters = {game: 143, MDP: 41}
> progress 72.0%, elapsed 179 s, estimated 248 s, iters = {game: 145, MDP: 43}
> progress 74.4%, elapsed 182 s, estimated 245 s, iters = {game: 148, MDP: 43}
> progress 76.0%, elapsed 186 s, estimated 245 s, iters = {game: 150, MDP: 44}
> progress 78.4%, elapsed 190 s, estimated 243 s, iters = {game: 153, MDP: 44}
> progress 80.0%, elapsed 194 s, estimated 243 s, iters = {game: 155, MDP: 45}
> progress 80.8%, elapsed 197 s, estimated 244 s, iters = {game: 157, MDP: 46}
> progress 84.0%, elapsed 202 s, estimated 240 s, iters = {game: 160, MDP: 46}
> progress 84.8%, elapsed 205 s, estimated 242 s, iters = {game: 162, MDP: 47}
> progress 88.0%, elapsed 208 s, estimated 236 s, iters = {game: 164, MDP: 47}
> progress 90.4%, elapsed 211 s, estimated 234 s, iters = {game: 166, MDP: 48}
--------------------
Policy tree summary:
found 120 satisfying policies for 30000/30000 family members (100.0%)
policy tree has 168 nodes, 120 of them are leaves:
	  solvable leaves: 120 (avg.size: 250.0)
	unsolvable leaves: 0 (avg.size: NA)
	 singleton leaves: 0
--------------------
2024-01-16 22:05:37,812 - policy_tree.py - post-processing the policy tree...
2024-01-16 22:05:37,812 - policy_tree.py - merging SAT siblings solved by non-exclusively compatible policies...
2024-01-16 22:05:56,964 - policy_tree.py - additional 231 MDPs were model checked
2024-01-16 22:05:56,964 - policy_tree.py - removed 106 nodes
2024-01-16 22:05:56,964 - policy_tree.py - merging all exclusively compatible policies...
2024-01-16 22:05:56,996 - policy_tree.py - removed 3 policies
2024-01-16 22:05:56,996 - policy_tree.py - reducing tree height...
2024-01-16 22:05:56,996 - policy_tree.py - removed 5 nodes
2024-01-16 22:05:56,996 - policy_tree.py - merging siblings that have the same solution...
2024-01-16 22:05:56,996 - policy_tree.py - removed 1 nodes
2024-01-16 22:05:56,997 - policy_tree.py - postprocessing took 19 s
--------------------
Policy tree summary:
found 36 satisfying policies for 30000/30000 family members (100.0%)
policy tree has 56 nodes, 38 of them are leaves:
	  solvable leaves: 38 (avg.size: 789.5)
	unsolvable leaves: 0 (avg.size: NA)
	 singleton leaves: 0
--------------------
2024-01-16 22:05:57,005 - synthesizer.py - evaluation finished
--------------------
Synthesis summary:
constraint 1: P>9/10 [F ((x = 8) & (y = 8))]

method: AR (policy tree), synthesis time: 233.58 s
number of holes: 6, family size: 30000, quotient: 149659 states / 723570 actions
explored: 100 %
Game stats: avg MDP size: 125548, iterations: 168
MDP stats: avg MDP size: 139489, iterations: 48

satisfied 30000/30000 members (100%)
--------------------
model info:	states	choices	MDPs	states*MDPs	SAT MDPs	SAT %
149659 723570 30000 4489770000 30000 100.0
synthesis info:	time	nodes	nodes (merged)	leaves	leaves (merged)	leaves (merged) / MDPs %	policies	policies (merged)	policies (merged) / SAT %	pp time	pp time %	game iters	MDP iters	iters/MDPs %
233 168 56 120 38 0.13 120 36 0.12 19 8.15 168 48 0.72

