2024-01-17 18:27:40,643 - cli.py - This is Paynt version 0.1.0.
2024-01-17 18:27:40,643 - sketch.py - loading sketch from /home/fpmk/research/synthesis/experiments/../models/mdp/cav24/dodge-8-mod2-pull-30/sketch.templ ...
2024-01-17 18:27:40,643 - sketch.py - assuming sketch in PRISM format...
2024-01-17 18:27:40,652 - prism_parser.py - PRISM model type: MDP
2024-01-17 18:27:40,652 - prism_parser.py - processing hole definitions...
2024-01-17 18:27:40,652 - prism_parser.py - loading properties from /home/fpmk/research/synthesis/experiments/../models/mdp/cav24/dodge-8-mod2-pull-30/sketch.props ...
2024-01-17 18:27:40,653 - prism_parser.py - found the following specification: constraints: P>9/10 [F ((x = 8) & (y = 8))]; 
2024-01-17 18:27:40,653 - jani.py - constructing JANI program...
2024-01-17 18:27:40,657 - jani.py - constructing the quotient...
2024-01-17 18:27:41,835 - jani.py - associating choices of the quotient with hole assignments...
2024-01-17 18:27:42,306 - sketch.py - sketch parsing OK
2024-01-17 18:27:42,306 - sketch.py - constructed explicit quotient having 149659 states and 723570 actions
2024-01-17 18:27:42,306 - sketch.py - found the following specification constraints: P>9/10 [F ((x = 8) & (y = 8))]; 
2024-01-17 18:27:42,572 - mdp_family.py - associating choices with action labels...
2024-01-17 18:27:43,593 - synthesizer.py - evaluation initiated, design space: 30000
> progress 0.0%, elapsed 4 s, estimated 4726079 s (54 days), iters = {game: 2, MDP: 1}
> progress 4.0%, elapsed 7 s, estimated 196 s, iters = {game: 4, MDP: 2}
> progress 16.0%, elapsed 12 s, estimated 77 s, iters = {game: 7, MDP: 2}
> progress 16.8%, elapsed 15 s, estimated 93 s, iters = {game: 9, MDP: 3}
> progress 19.2%, elapsed 19 s, estimated 103 s, iters = {game: 12, MDP: 3}
> progress 20.0%, elapsed 23 s, estimated 118 s, iters = {game: 14, MDP: 4}
> progress 20.8%, elapsed 26 s, estimated 128 s, iters = {game: 16, MDP: 5}
> progress 23.2%, elapsed 30 s, estimated 131 s, iters = {game: 19, MDP: 5}
> progress 24.0%, elapsed 34 s, estimated 141 s, iters = {game: 21, MDP: 6}
> progress 26.4%, elapsed 37 s, estimated 143 s, iters = {game: 24, MDP: 6}
> progress 28.0%, elapsed 41 s, estimated 146 s, iters = {game: 26, MDP: 6}
> progress 28.799%, elapsed 44 s, estimated 153 s, iters = {game: 28, MDP: 7}
> progress 31.2%, elapsed 48 s, estimated 154 s, iters = {game: 31, MDP: 7}
> progress 32.0%, elapsed 51 s, estimated 161 s, iters = {game: 33, MDP: 8}
> progress 34.4%, elapsed 55 s, estimated 161 s, iters = {game: 36, MDP: 8}
> progress 36.0%, elapsed 58 s, estimated 162 s, iters = {game: 38, MDP: 8}
> progress 36.8%, elapsed 61 s, estimated 167 s, iters = {game: 40, MDP: 9}
> progress 39.2%, elapsed 65 s, estimated 167 s, iters = {game: 43, MDP: 9}
> progress 40.0%, elapsed 69 s, estimated 174 s, iters = {game: 45, MDP: 10}
> progress 40.8%, elapsed 72 s, estimated 178 s, iters = {game: 47, MDP: 11}
> progress 43.2%, elapsed 76 s, estimated 178 s, iters = {game: 50, MDP: 11}
> progress 43.52%, elapsed 80 s, estimated 185 s, iters = {game: 53, MDP: 12}
> progress 44.0%, elapsed 84 s, estimated 191 s, iters = {game: 56, MDP: 12}
> progress 45.6%, elapsed 88 s, estimated 194 s, iters = {game: 59, MDP: 13}
> progress 48.0%, elapsed 92 s, estimated 193 s, iters = {game: 62, MDP: 13}
> progress 48.8%, elapsed 96 s, estimated 196 s, iters = {game: 64, MDP: 14}
> progress 51.2%, elapsed 100 s, estimated 195 s, iters = {game: 67, MDP: 14}
> progress 51.52%, elapsed 103 s, estimated 200 s, iters = {game: 70, MDP: 15}
> progress 52.0%, elapsed 107 s, estimated 206 s, iters = {game: 73, MDP: 15}
> progress 52.8%, elapsed 110 s, estimated 209 s, iters = {game: 75, MDP: 16}
> progress 55.2%, elapsed 114 s, estimated 207 s, iters = {game: 78, MDP: 16}
> progress 56.0%, elapsed 117 s, estimated 210 s, iters = {game: 80, MDP: 17}
> progress 58.399%, elapsed 121 s, estimated 208 s, iters = {game: 83, MDP: 17}
> progress 60.0%, elapsed 125 s, estimated 208 s, iters = {game: 85, MDP: 17}
> progress 60.0%, elapsed 129 s, estimated 215 s, iters = {game: 87, MDP: 19}
> progress 62.4%, elapsed 132 s, estimated 213 s, iters = {game: 90, MDP: 19}
> progress 64.0%, elapsed 136 s, estimated 212 s, iters = {game: 92, MDP: 19}
> progress 64.8%, elapsed 139 s, estimated 214 s, iters = {game: 94, MDP: 20}
> progress 67.2%, elapsed 143 s, estimated 213 s, iters = {game: 97, MDP: 20}
> progress 68.0%, elapsed 146 s, estimated 215 s, iters = {game: 99, MDP: 21}
> progress 70.4%, elapsed 150 s, estimated 213 s, iters = {game: 102, MDP: 21}
> progress 72.0%, elapsed 153 s, estimated 213 s, iters = {game: 104, MDP: 21}
> progress 72.8%, elapsed 156 s, estimated 215 s, iters = {game: 106, MDP: 22}
> progress 75.2%, elapsed 160 s, estimated 213 s, iters = {game: 109, MDP: 22}
> progress 76.0%, elapsed 164 s, estimated 215 s, iters = {game: 111, MDP: 23}
> progress 78.4%, elapsed 167 s, estimated 214 s, iters = {game: 114, MDP: 23}
> progress 80.0%, elapsed 170 s, estimated 213 s, iters = {game: 116, MDP: 23}
> progress 80.0%, elapsed 174 s, estimated 218 s, iters = {game: 118, MDP: 25}
> progress 82.4%, elapsed 178 s, estimated 216 s, iters = {game: 121, MDP: 25}
> progress 84.0%, elapsed 181 s, estimated 216 s, iters = {game: 123, MDP: 25}
> progress 85.6%, elapsed 186 s, estimated 217 s, iters = {game: 126, MDP: 26}
> progress 88.0%, elapsed 190 s, estimated 216 s, iters = {game: 129, MDP: 26}
> progress 88.8%, elapsed 193 s, estimated 218 s, iters = {game: 131, MDP: 27}
> progress 91.2%, elapsed 197 s, estimated 216 s, iters = {game: 134, MDP: 27}
> progress 91.52%, elapsed 200 s, estimated 219 s, iters = {game: 137, MDP: 28}
> progress 92.0%, elapsed 204 s, estimated 222 s, iters = {game: 140, MDP: 28}
> progress 92.8%, elapsed 207 s, estimated 223 s, iters = {game: 142, MDP: 29}
> progress 95.2%, elapsed 211 s, estimated 222 s, iters = {game: 145, MDP: 29}
> progress 96.0%, elapsed 215 s, estimated 224 s, iters = {game: 147, MDP: 30}
> progress 98.4%, elapsed 219 s, estimated 223 s, iters = {game: 150, MDP: 30}
--------------------
Policy tree summary:
found 121 satisfying policies for 30000/30000 family members (100.0%)
policy tree has 151 nodes, 121 of them are leaves:
	  solvable leaves: 121 (avg.size: 247.9)
	unsolvable leaves: 0 (avg.size: NA)
	 singleton leaves: 0
--------------------
2024-01-17 18:31:24,600 - policy_tree.py - post-processing the policy tree...
2024-01-17 18:31:24,600 - policy_tree.py - merging SAT siblings solved by non-exclusively compatible policies...
2024-01-17 18:31:49,376 - policy_tree.py - additional 278 MDPs were model checked
2024-01-17 18:31:49,376 - policy_tree.py - removed 64 nodes
2024-01-17 18:31:49,376 - policy_tree.py - merging all exclusively compatible policies...
2024-01-17 18:31:49,379 - policy_tree.py - removed 0 policies
2024-01-17 18:31:49,379 - policy_tree.py - reducing tree height...
2024-01-17 18:31:49,379 - policy_tree.py - removed 0 nodes
2024-01-17 18:31:49,379 - policy_tree.py - merging siblings that have the same solution...
2024-01-17 18:31:49,379 - policy_tree.py - removed 0 nodes
2024-01-17 18:31:49,379 - policy_tree.py - postprocessing took 24 s
--------------------
Policy tree summary:
found 60 satisfying policies for 30000/30000 family members (100.0%)
policy tree has 87 nodes, 60 of them are leaves:
	  solvable leaves: 60 (avg.size: 500.0)
	unsolvable leaves: 0 (avg.size: NA)
	 singleton leaves: 0
--------------------
2024-01-17 18:31:49,387 - synthesizer.py - evaluation finished
--------------------
Synthesis summary:
constraint 1: P>9/10 [F ((x = 8) & (y = 8))]

method: AR (policy tree), synthesis time: 245.79 s
number of holes: 6, family size: 30000, quotient: 149659 states / 723570 actions
explored: 100 %
Game stats: avg MDP size: 149408, iterations: 151
MDP stats: avg MDP size: 149625, iterations: 30

satisfied 30000/30000 members (100%)
--------------------
model info:	states	choices	MDPs	states*MDPs	SAT MDPs	SAT %
149659 723570 30000 4489770000 30000 100.0
synthesis info:	time	nodes	nodes (merged)	leaves	leaves (merged)	leaves (merged) / MDPs %	policies	policies (merged)	policies (merged) / SAT %	pp time	pp time %	game iters	MDP iters	iters/MDPs %
245 151 87 121 60 0.2 121 60 0.2 24 9.8 151 30 0.6

