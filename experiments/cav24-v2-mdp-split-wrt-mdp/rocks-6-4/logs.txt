2024-01-17 18:19:45,715 - cli.py - This is Paynt version 0.1.0.
2024-01-17 18:19:45,715 - sketch.py - loading sketch from /home/fpmk/research/synthesis/experiments/../models/mdp/cav24/rocks-6-4/sketch.templ ...
2024-01-17 18:19:45,715 - sketch.py - assuming sketch in PRISM format...
2024-01-17 18:19:45,743 - prism_parser.py - PRISM model type: MDP
2024-01-17 18:19:45,743 - prism_parser.py - processing hole definitions...
2024-01-17 18:19:45,744 - prism_parser.py - loading properties from /home/fpmk/research/synthesis/experiments/../models/mdp/cav24/rocks-6-4/sketch.props ...
2024-01-17 18:19:45,748 - prism_parser.py - found the following specification: constraints: P>=1 [F (((visit1 & visit2) & visit3) & visit4)]; 
2024-01-17 18:19:45,748 - jani.py - constructing JANI program...
2024-01-17 18:19:45,756 - jani.py - constructing the quotient...
2024-01-17 18:19:45,838 - jani.py - associating choices of the quotient with hole assignments...
2024-01-17 18:19:45,842 - sketch.py - sketch parsing OK
2024-01-17 18:19:45,842 - sketch.py - constructed explicit quotient having 2736 states and 6660 actions
2024-01-17 18:19:45,842 - sketch.py - found the following specification constraints: P>=1 [F (((visit1 & visit2) & visit3) & visit4)]; 
2024-01-17 18:19:45,843 - mdp_family.py - associating choices with action labels...
2024-01-17 18:19:45,861 - synthesizer.py - evaluation initiated, design space: 6561
> progress 3.475%, elapsed 3 s, estimated 86 s, iters = {game: 119, MDP: 334}
> progress 6.767%, elapsed 6 s, estimated 88 s, iters = {game: 227, MDP: 650}
> progress 9.876%, elapsed 9 s, estimated 91 s, iters = {game: 328, MDP: 947}
> progress 13.183%, elapsed 12 s, estimated 91 s, iters = {game: 438, MDP: 1272}
> progress 16.689%, elapsed 15 s, estimated 90 s, iters = {game: 552, MDP: 1604}
> progress 19.753%, elapsed 18 s, estimated 91 s, iters = {game: 654, MDP: 1893}
> progress 22.633%, elapsed 21 s, estimated 92 s, iters = {game: 749, MDP: 2173}
> progress 25.56%, elapsed 24 s, estimated 94 s, iters = {game: 843, MDP: 2455}
> progress 28.257%, elapsed 27 s, estimated 95 s, iters = {game: 930, MDP: 2711}
> progress 30.864%, elapsed 30 s, estimated 97 s, iters = {game: 1015, MDP: 2963}
> progress 33.516%, elapsed 33 s, estimated 98 s, iters = {game: 1106, MDP: 3224}
> progress 36.854%, elapsed 36 s, estimated 97 s, iters = {game: 1213, MDP: 3538}
> progress 40.009%, elapsed 39 s, estimated 97 s, iters = {game: 1317, MDP: 3843}
> progress 43.164%, elapsed 42 s, estimated 97 s, iters = {game: 1419, MDP: 4142}
> progress 46.181%, elapsed 45 s, estimated 97 s, iters = {game: 1520, MDP: 4437}
> progress 49.154%, elapsed 48 s, estimated 97 s, iters = {game: 1617, MDP: 4717}
> progress 52.263%, elapsed 51 s, estimated 97 s, iters = {game: 1719, MDP: 5015}
> progress 55.144%, elapsed 54 s, estimated 98 s, iters = {game: 1812, MDP: 5293}
> progress 57.613%, elapsed 57 s, estimated 99 s, iters = {game: 1895, MDP: 5532}
> progress 60.219%, elapsed 60 s, estimated 99 s, iters = {game: 1979, MDP: 5781}
> progress 62.962%, elapsed 63 s, estimated 100 s, iters = {game: 2068, MDP: 6046}
> progress 65.432%, elapsed 66 s, estimated 101 s, iters = {game: 2150, MDP: 6282}
> progress 68.038%, elapsed 69 s, estimated 101 s, iters = {game: 2237, MDP: 6540}
> progress 70.781%, elapsed 72 s, estimated 102 s, iters = {game: 2327, MDP: 6810}
> progress 73.662%, elapsed 75 s, estimated 102 s, iters = {game: 2420, MDP: 7092}
> progress 76.543%, elapsed 78 s, estimated 102 s, iters = {game: 2515, MDP: 7376}
> progress 79.301%, elapsed 81 s, estimated 102 s, iters = {game: 2606, MDP: 7650}
> progress 81.893%, elapsed 84 s, estimated 102 s, iters = {game: 2691, MDP: 7903}
> progress 84.636%, elapsed 87 s, estimated 103 s, iters = {game: 2779, MDP: 8171}
> progress 87.242%, elapsed 90 s, estimated 103 s, iters = {game: 2864, MDP: 8427}
> progress 89.894%, elapsed 93 s, estimated 103 s, iters = {game: 2953, MDP: 8686}
> progress 92.455%, elapsed 96 s, estimated 104 s, iters = {game: 3036, MDP: 8933}
> progress 94.741%, elapsed 99 s, estimated 104 s, iters = {game: 3111, MDP: 9150}
> progress 97.21%, elapsed 102 s, estimated 105 s, iters = {game: 3192, MDP: 9389}
> progress 99.954%, elapsed 105 s, estimated 105 s, iters = {game: 3280, MDP: 9653}
--------------------
Policy tree summary:
found 6469 satisfying policies for 6561/6561 family members (100.0%)
policy tree has 9703 nodes, 6469 of them are leaves:
	  solvable leaves: 6469 (avg.size: 1.0)
	unsolvable leaves: 0 (avg.size: NA)
	 singleton leaves: 6423
--------------------
2024-01-17 18:21:31,591 - policy_tree.py - post-processing the policy tree...
2024-01-17 18:21:31,591 - policy_tree.py - merging SAT siblings solved by non-exclusively compatible policies...
2024-01-17 18:21:44,958 - policy_tree.py - additional 11467 MDPs were model checked
2024-01-17 18:21:44,958 - policy_tree.py - removed 5724 nodes
2024-01-17 18:21:44,958 - policy_tree.py - merging all exclusively compatible policies...
2024-01-17 18:21:46,765 - policy_tree.py - removed 42 policies
2024-01-17 18:21:46,765 - policy_tree.py - reducing tree height...
2024-01-17 18:21:46,780 - policy_tree.py - removed 0 nodes
2024-01-17 18:21:46,780 - policy_tree.py - merging siblings that have the same solution...
2024-01-17 18:21:46,795 - policy_tree.py - removed 0 nodes
2024-01-17 18:21:46,795 - policy_tree.py - postprocessing took 15 s
--------------------
Policy tree summary:
found 2492 satisfying policies for 6561/6561 family members (100.0%)
policy tree has 3979 nodes, 2534 of them are leaves:
	  solvable leaves: 2534 (avg.size: 2.6)
	unsolvable leaves: 0 (avg.size: NA)
	 singleton leaves: 358
--------------------
2024-01-17 18:21:46,894 - synthesizer.py - evaluation finished
--------------------
Synthesis summary:
constraint 1: P>=1 [F (((visit1 & visit2) & visit3) & visit4)]

method: AR (policy tree), synthesis time: 121.03 s
number of holes: 8, family size: 6561, quotient: 2736 states / 6660 actions
explored: 100 %
Game stats: avg MDP size: 2639, iterations: 3280
MDP stats: avg MDP size: 2629, iterations: 9657

satisfied 6561/6561 members (100%)
--------------------
model info:	states	choices	MDPs	states*MDPs	SAT MDPs	SAT %
2736 6660 6561 17950896 6561 100.0
synthesis info:	time	nodes	nodes (merged)	leaves	leaves (merged)	leaves (merged) / MDPs %	policies	policies (merged)	policies (merged) / SAT %	pp time	pp time %	game iters	MDP iters	iters/MDPs %
121 9703 3979 6469 2534 38.62 6469 2492 37.98 15 12.4 3280 9657 197.18

