2024-01-09 22:19:29,722 - cli.py - This is Paynt version 0.1.0.
2024-01-09 22:19:29,722 - sketch.py - loading sketch from /home/fpmk/research/synthesis/experiments/../models/mdp/cav24/rocks-6-4/sketch.templ ...
2024-01-09 22:19:29,722 - sketch.py - assuming sketch in PRISM format...
2024-01-09 22:19:29,725 - prism_parser.py - PRISM model type: MDP
2024-01-09 22:19:29,725 - prism_parser.py - processing hole definitions...
2024-01-09 22:19:29,725 - prism_parser.py - loading properties from /home/fpmk/research/synthesis/experiments/../models/mdp/cav24/rocks-6-4/sketch.props ...
2024-01-09 22:19:29,726 - prism_parser.py - found the following specification: constraints: P>=1 [F (((visit1 & visit2) & visit3) & visit4)]; 
2024-01-09 22:19:29,726 - jani.py - constructing JANI program...
2024-01-09 22:19:29,728 - jani.py - constructing the quotient...
2024-01-09 22:19:29,750 - jani.py - associating choices of the quotient with hole assignments...
2024-01-09 22:19:29,754 - sketch.py - sketch parsing OK
2024-01-09 22:19:29,754 - sketch.py - constructed explicit quotient having 2736 states and 6660 actions
2024-01-09 22:19:29,754 - sketch.py - found the following specification constraints: P>=1 [F (((visit1 & visit2) & visit3) & visit4)]; 
2024-01-09 22:19:29,755 - mdp_family.py - associating choices with action labels...
2024-01-09 22:19:29,791 - synthesizer.py - evaluation initiated, design space: 6561
> Progress 3.246%, elapsed 3 s, estimated 92 s, iters = {game: 111, MDP: 323}
> Progress 6.721%, elapsed 6 s, estimated 89 s, iters = {game: 225, MDP: 665}
> Progress 9.83%, elapsed 9 s, estimated 91 s, iters = {game: 326, MDP: 972}
> Progress 12.68%, elapsed 12 s, estimated 94 s, iters = {game: 421, MDP: 1254}
> Progress 15.592%, elapsed 15 s, estimated 96 s, iters = {game: 516, MDP: 1538}
> Progress 18.533%, elapsed 18 s, estimated 97 s, iters = {game: 614, MDP: 1831}
> Progress 21.338%, elapsed 21 s, estimated 98 s, iters = {game: 703, MDP: 2104}
> Progress 24.417%, elapsed 24 s, estimated 98 s, iters = {game: 805, MDP: 2406}
> Progress 27.892%, elapsed 27 s, estimated 96 s, iters = {game: 919, MDP: 2748}
> Progress 31.367%, elapsed 30 s, estimated 95 s, iters = {game: 1033, MDP: 3090}
> Progress 34.476%, elapsed 33 s, estimated 95 s, iters = {game: 1136, MDP: 3399}
> Progress 37.219%, elapsed 36 s, estimated 96 s, iters = {game: 1227, MDP: 3668}
> Progress 40.192%, elapsed 39 s, estimated 97 s, iters = {game: 1322, MDP: 3959}
> Progress 42.798%, elapsed 42 s, estimated 98 s, iters = {game: 1408, MDP: 4215}
> Progress 45.358%, elapsed 45 s, estimated 99 s, iters = {game: 1493, MDP: 4468}
> Progress 48.148%, elapsed 48 s, estimated 99 s, iters = {game: 1582, MDP: 4740}
> Progress 50.937%, elapsed 51 s, estimated 100 s, iters = {game: 1675, MDP: 5016}
> Progress 53.497%, elapsed 54 s, estimated 101 s, iters = {game: 1760, MDP: 5269}
> Progress 56.104%, elapsed 57 s, estimated 101 s, iters = {game: 1845, MDP: 5525}
> Progress 58.984%, elapsed 60 s, estimated 102 s, iters = {game: 1938, MDP: 5807}
> Progress 62.002%, elapsed 63 s, estimated 101 s, iters = {game: 2037, MDP: 6104}
> Progress 64.837%, elapsed 66 s, estimated 102 s, iters = {game: 2130, MDP: 6383}
> Progress 67.764%, elapsed 69 s, estimated 102 s, iters = {game: 2227, MDP: 6672}
> Progress 71.239%, elapsed 72 s, estimated 101 s, iters = {game: 2342, MDP: 7017}
> Progress 74.897%, elapsed 75 s, estimated 100 s, iters = {game: 2460, MDP: 7373}
> Progress 78.052%, elapsed 78 s, estimated 100 s, iters = {game: 2566, MDP: 7686}
> Progress 81.069%, elapsed 81 s, estimated 100 s, iters = {game: 2664, MDP: 7982}
> Progress 84.362%, elapsed 84 s, estimated 99 s, iters = {game: 2770, MDP: 8304}
> Progress 87.059%, elapsed 87 s, estimated 100 s, iters = {game: 2859, MDP: 8570}
> Progress 90.169%, elapsed 90 s, estimated 100 s, iters = {game: 2963, MDP: 8878}
> Progress 93.842%, elapsed 93 s, estimated 99 s, iters = {game: 3083, MDP: 9241}
> Progress 97.53%, elapsed 96 s, estimated 98 s, iters = {game: 3202, MDP: 9600}
--------------------
Policy tree summary:
found 6561 satisfying policies for 6561/6561 family members (100.0%)
policy tree has 9841 nodes, 6561 of them are leaves:
	  solvable leaves: 6561 (avg.size: 1.0)
	unsolvable leaves: 0 (avg.size: NA)
	 singleton leaves: 6561
--------------------
2024-01-09 22:21:08,557 - policy_tree.py - post-processing the policy tree...
2024-01-09 22:21:08,557 - policy_tree.py - merging SAT siblings solved by non-exclusively compatible policies...
2024-01-09 22:21:23,236 - policy_tree.py - additional 12120 MDPs were model checked
2024-01-09 22:21:23,236 - policy_tree.py - removed 6358 nodes
2024-01-09 22:21:23,236 - policy_tree.py - merging all exclusively compatible policies...
2024-01-09 22:21:24,879 - policy_tree.py - removed 49 policies
2024-01-09 22:21:24,880 - policy_tree.py - reducing tree height...
2024-01-09 22:21:24,892 - policy_tree.py - removed 0 nodes
2024-01-09 22:21:24,892 - policy_tree.py - merging siblings that have the same solution...
2024-01-09 22:21:24,904 - policy_tree.py - removed 0 nodes
2024-01-09 22:21:24,904 - policy_tree.py - postprocessing took 16 s
--------------------
Policy tree summary:
found 2238 satisfying policies for 6561/6561 family members (100.0%)
policy tree has 3483 nodes, 2287 of them are leaves:
	  solvable leaves: 2287 (avg.size: 2.9)
	unsolvable leaves: 0 (avg.size: NA)
	 singleton leaves: 103
--------------------
2024-01-09 22:21:25,018 - synthesizer.py - evaluation finished
--------------------
Synthesis summary:
constraint 1: P>=1 [F (((visit1 & visit2) & visit3) & visit4)]

method: AR (policy tree), synthesis time: 115.23 s
number of holes: 8, family size: 6561, quotient: 2736 states / 6660 actions
explored: 100 %
Game stats: avg MDP size: 2639, iterations: 3280
MDP stats: avg MDP size: 2629, iterations: 9841

satisfied 6561/6561 members (100%)
--------------------
model info:	states	choices	MDPs	states*MDPs	SAT MDPs	SAT %
2736 6660 6561 17950896 6561 100.0
synt info:	time	nodes	nodes (merged)	policies	policies (merged)	policies(merged) / SAT %	game iters	MDP iters	iters/MDPs
115 9841 3483 6561 2238 34.1 3280 9841 2.0

