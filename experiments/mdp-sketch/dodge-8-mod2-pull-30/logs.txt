2024-01-08 13:48:13,982 - cli.py - This is Paynt version 0.1.0.
2024-01-08 13:48:13,982 - sketch.py - loading sketch from /home/fpmk/research/synthesis/experiments/../models/mdp/sketches/dodge/8-mod2-pull-30/sketch.templ ...
2024-01-08 13:48:13,982 - sketch.py - assuming sketch in PRISM format...
2024-01-08 13:48:13,985 - prism_parser.py - PRISM model type: MDP
2024-01-08 13:48:13,985 - prism_parser.py - processing hole definitions...
2024-01-08 13:48:13,985 - prism_parser.py - loading properties from /home/fpmk/research/synthesis/experiments/../models/mdp/sketches/dodge/8-mod2-pull-30/sketch.props ...
2024-01-08 13:48:13,986 - prism_parser.py - found the following specification: constraints: P>9/10 [F ((x = 8) & (y = 8))]; 
2024-01-08 13:48:13,986 - jani.py - constructing JANI program...
2024-01-08 13:48:13,990 - jani.py - constructing the quotient...
2024-01-08 13:48:15,088 - jani.py - associating choices of the quotient with hole assignments...
2024-01-08 13:48:15,552 - sketch.py - sketch parsing OK
2024-01-08 13:48:15,553 - sketch.py - constructed explicit quotient having 149659 states and 723570 actions
2024-01-08 13:48:15,553 - sketch.py - found the following specification constraints: P>9/10 [F ((x = 8) & (y = 8))]; 
2024-01-08 13:48:15,835 - mdp_family.py - associating choices with action labels...
2024-01-08 13:48:16,724 - synthesizer.py - evaluation initiated, design space: 30000
> Progress 0.0%, elapsed 4 s, estimated 4522634 s (52 days), iters = {game: 2, MDP: 1}
> Progress 4.0%, elapsed 7 s, estimated 191 s, iters = {game: 4, MDP: 3}
> Progress 12.0%, elapsed 10 s, estimated 89 s, iters = {game: 6, MDP: 6}
> Progress 16.0%, elapsed 13 s, estimated 87 s, iters = {game: 8, MDP: 7}
> Progress 18.4%, elapsed 18 s, estimated 99 s, iters = {game: 11, MDP: 10}
> Progress 20.0%, elapsed 21 s, estimated 107 s, iters = {game: 13, MDP: 12}
> Progress 24.0%, elapsed 24 s, estimated 102 s, iters = {game: 15, MDP: 14}
> Progress 32.0%, elapsed 27 s, estimated 86 s, iters = {game: 17, MDP: 17}
> Progress 36.0%, elapsed 30 s, estimated 85 s, iters = {game: 19, MDP: 18}
> Progress 38.4%, elapsed 35 s, estimated 91 s, iters = {game: 22, MDP: 21}
> Progress 40.0%, elapsed 38 s, estimated 95 s, iters = {game: 24, MDP: 23}
> Progress 44.0%, elapsed 41 s, estimated 94 s, iters = {game: 26, MDP: 25}
> Progress 52.0%, elapsed 44 s, estimated 85 s, iters = {game: 28, MDP: 28}
> Progress 56.0%, elapsed 47 s, estimated 85 s, iters = {game: 30, MDP: 29}
> Progress 57.599%, elapsed 50 s, estimated 88 s, iters = {game: 32, MDP: 32}
> Progress 60.0%, elapsed 55 s, estimated 92 s, iters = {game: 35, MDP: 34}
> Progress 64.0%, elapsed 58 s, estimated 91 s, iters = {game: 37, MDP: 36}
> Progress 72.0%, elapsed 61 s, estimated 85 s, iters = {game: 39, MDP: 38}
> Progress 76.0%, elapsed 65 s, estimated 85 s, iters = {game: 41, MDP: 40}
> Progress 78.4%, elapsed 69 s, estimated 88 s, iters = {game: 44, MDP: 43}
> Progress 80.0%, elapsed 72 s, estimated 90 s, iters = {game: 46, MDP: 45}
> Progress 80.0%, elapsed 76 s, estimated 95 s, iters = {game: 48, MDP: 47}
> Progress 82.4%, elapsed 80 s, estimated 97 s, iters = {game: 51, MDP: 50}
> Progress 84.0%, elapsed 83 s, estimated 99 s, iters = {game: 53, MDP: 53}
> Progress 85.6%, elapsed 87 s, estimated 102 s, iters = {game: 56, MDP: 55}
> Progress 88.0%, elapsed 91 s, estimated 104 s, iters = {game: 59, MDP: 58}
> Progress 88.8%, elapsed 94 s, estimated 106 s, iters = {game: 61, MDP: 61}
> Progress 91.2%, elapsed 98 s, estimated 108 s, iters = {game: 64, MDP: 63}
> Progress 92.0%, elapsed 101 s, estimated 110 s, iters = {game: 66, MDP: 65}
> Progress 94.4%, elapsed 105 s, estimated 112 s, iters = {game: 69, MDP: 68}
> Progress 96.0%, elapsed 109 s, estimated 113 s, iters = {game: 71, MDP: 71}
> Progress 96.8%, elapsed 112 s, estimated 115 s, iters = {game: 73, MDP: 72}
> Progress 99.2%, elapsed 116 s, estimated 117 s, iters = {game: 76, MDP: 75}
--------------------
Policy tree summary:
found 61 satisfying policies for 30000/30000 family members (100.0%)
policy tree has 76 nodes, 61 of them are leaves:
	  solvable leaves: 61 (avg.size: 491.8)
	unsolvable leaves: 0 (avg.size: NA)
	 singleton leaves: 0
2024-01-08 13:50:13,395 - policy_tree.py - post-processing the policy tree...
2024-01-08 13:50:13,395 - policy_tree.py - merging unsat siblings...
2024-01-08 13:50:13,395 - policy_tree.py - merged 0 pairs
2024-01-08 13:50:13,395 - policy_tree.py - merging compatible siblings...
2024-01-08 13:50:25,051 - policy_tree.py - merged 32 pairs
--------------------
Policy tree summary:
found 29 satisfying policies for 30000/30000 family members (100.0%)
policy tree has 44 nodes, 29 of them are leaves:
	  solvable leaves: 29 (avg.size: 1034.5)
	unsolvable leaves: 0 (avg.size: NA)
	 singleton leaves: 0
2024-01-08 13:50:25,066 - synthesizer.py - evaluation finished
--------------------
Synthesis summary:
constraint 1: P>9/10 [F ((x = 8) & (y = 8))]

method: AR (policy tree), synthesis time: 128.34 s
number of holes: 6, family size: 30000, quotient: 149659 states / 723570 actions
explored: 100 %
Game stats: avg MDP size: 149463, iterations: 76
MDP stats: avg MDP size: 102480, iterations: 76

satisfied 30000/30000 members (100%)
--------------------
