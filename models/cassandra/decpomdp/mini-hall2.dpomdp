agents: 2 
#
discount: 0.95
#
values: reward
#
states: 13
#  
start:
0.083337 0.083333 0.083333 0.083333 0.083333 0.083333 0.083333 0.083333 0.083333 0.083333 0.083333 0.083333 0.0 
#
actions: 
1
3
#
observations: 
1
9
#
# Transition Probabilities
T: 0 0 : 0 : 0 : 1.000000
T: 0 1 : 0 : 1 : 1.000000
T: 0 2 : 0 : 3 : 1.000000
T: 0 0 : 1 : 1 : 1.000000
T: 0 1 : 1 : 2 : 1.000000
T: 0 2 : 1 : 0 : 1.000000
T: 0 0 : 2 : 6 : 1.000000
T: 0 1 : 2 : 3 : 1.000000
T: 0 2 : 2 : 1 : 1.000000
T: 0 0 : 3 : 3 : 1.000000
T: 0 1 : 3 : 0 : 1.000000
T: 0 2 : 3 : 2 : 1.000000
T: 0 0 : 4 : 0 : 1.000000
T: 0 1 : 4 : 5 : 1.000000
T: 0 2 : 4 : 7 : 1.000000
T: 0 0 : 5 : 9 : 1.000000
T: 0 1 : 5 : 6 : 1.000000
T: 0 2 : 5 : 4 : 1.000000
T: 0 0 : 6 : 0 : 1.000000
T: 0 1 : 6 : 7 : 1.000000
T: 0 2 : 6 : 5 : 1.000000
T: 0 0 : 7 : 7 : 1.000000
T: 0 1 : 7 : 4 : 1.000000
T: 0 2 : 7 : 6 : 1.000000
T: 0 0 : 8 : 8 : 1.000000
T: 0 1 : 8 : 9 : 1.000000
T: 0 2 : 8 : 11 : 1.000000
T: 0 0 : 9 : 9 : 1.000000
T: 0 1 : 9 : 10 : 1.000000
T: 0 2 : 9 : 8 : 1.000000
T: 0 0 : 10 : 12 : 1.000000
T: 0 1 : 10 : 11 : 1.000000
T: 0 2 : 10 : 9 : 1.000000
T: 0 0 : 11 : 7 : 1.000000
T: 0 1 : 11 : 8 : 1.000000
T: 0 2 : 11 : 10 : 1.000000
#
T: 0 * : 12 :
0.083337 0.083333 0.083333 0.083333 0.083333 0.083333 0.083333 0.083333 0.083333 0.083333 0.083333 0.083333 0.0 
#
# Observation Probabilities
O: 0 * : 0 : 0 0 : 1.0
O: 0 * : 1 : 0 1 : 1.0
O: 0 * : 2 : 0 2 : 1.0
O: 0 * : 3 : 0 3 : 1.0
O: 0 * : 4 : 0 4 : 1.0
O: 0 * : 5 : 0 5 : 1.0
O: 0 * : 6 : 0 6 : 1.0
O: 0 * : 7 : 0 7 : 1.0
O: 0 * : 8 : 0 6 : 1.0
O: 0 * : 9 : 0 7 : 1.0
O: 0 * : 10 : 0 4 : 1.0
O: 0 * : 11 : 0 5 : 1.0
O: 0 * : 12 : 0 8 : 1.0
#
# Rewards
R: 0 * : * : 12 : * : 1.000000
#R: a a: s2 : * : * : 1
