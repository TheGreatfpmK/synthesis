# This is a Dec-POMDP (.dpomdp) file for the Dec-Tiger problem.
# For more detailed documentation, see example.dpomdp

# Allright, here we go!
#
#The agents. 
#----------
#Either 1) the number of agents: 
#   agents: %d
#or 2) a list of agent identifiers, e.g.:
#   agents: agent1_name, name-of-agent2, ... 
agents: 2 
#   discount: %f 
discount: 0.95
#.0
#   values: [ reward, cost ] 
values: reward
#   states: [ %d, <list of states> ] 
states: I hi-A lo-A C D plus1 minus1    
#
#Examples of this are:
#   start: 0.3 0.1 0.0 0.2 0.5
#   start: first-state
#   start: 5
#   start: uniform
#   start include: first-state third state
#   start include: 1 3
#   start exclude: fifth-state seventh-state
start include: I
#
#The actions declarations
#------------------------
#the  (number/list of) actions for each of the agents on a separate line
#   actions: 
#   [ %d, <list of actions> ] 
#   [ %d, <list of actions> ] 
#   ...
#   [ %d, <list of actions> ] 
actions:
a1
a b c
#the (number/list of) observations for each of the agents on a separate line
#   observations: 
#   [ %d, <list of observations> ]
#   [ %d, <list of observations> ]
#   ...
#   [ %d, <list of observations> ]
observations: 
o1
I A C D plus1 minus1
#Transition probabilities
#   T: <a1 a2...an> : <start-state> : <end-state> : %f
#or
#   T: <a1 a2...an> : <start-state> :
#   %f %f ... %f			    P(s_1'|ja,s) ... P(s_k'|ja,s)
#or
#   T: <a1 a2...an> :			    this is a |S| x |S| matrix
#   %f %f ... %f			    P(s_1'|ja,s_1) ... P(s_k'|ja,s_1)
#   %f %f ... %f			    ...
#   ...					    ...
#   %f %f ... %f			    P(s_1'|ja,s_k) ... P(s_k'|ja,s_k)
#or
#   T: <a1 a2...an> 
#   [ identity, uniform ]
T : a1 a : I : hi-A : 0.5
T : a1 b : I : hi-A : 0.5
T : a1 c : I : hi-A : 0.5
T : a1 a : I : lo-A : 0.5
T : a1 b : I : lo-A : 0.5
T : a1 c : I : lo-A : 0.5
#
T : a1 a : hi-A : C : 1.0
T : a1 b : hi-A : minus1 : 1.0
T : a1 c : hi-A : plus1 : 1.0
#
T : a1 a : lo-A : D : 1.0
T : a1 b : lo-A : plus1 : 1.0
T : a1 c : lo-A : minus1 : 1.0
#
T : a1 a : C : hi-A : 1.0
T : a1 b : C : I : 1.0
T : a1 c : C : I : 1.0
#
T : a1 a : D : lo-A : 1.0
T : a1 b : D : I : 1.0
T : a1 c : D : I : 1.0
#
T : a1 a : plus1 : I : 1.0
T : a1 b : plus1 : I : 1.0
T : a1 c : plus1 : I : 1.0
#
T : a1 a : minus1 : I : 1.0
T : a1 b : minus1 : I : 1.0
T : a1 c : minus1 : I : 1.0
#T:open-right open-right :
#uniform
#T: listen listen :
#identity 
#Observation probabilities
#    O: <a1 a2...an> : <end-state> : <o1 o2 ... om> : %f
#or
#    O: <a1 a2...an> : <end-state> :
#    %f %f ... %f	    P(jo_1|ja,s') ... P(jo_x|ja,s')
#or
#    O:<a1 a2...an> :	    - a |S|x|JO| matrix
#    %f %f ... %f	    P(jo_1|ja,s_1') ... P(jo_x|ja,s_1') 
#    %f %f ... %f	    ... 
#    ...		    ...
#    %f %f ... %f	    P(jo_1|ja,s_k') ... P(jo_x|ja,s_k') 
O : a1 a : I : o1 I : 1.0
O : a1 b : I : o1 I : 1.0
O : a1 c : I : o1 I : 1.0
#
O : a1 a : hi-A : o1 A : 1.0
O : a1 b : hi-A : o1 A : 1.0
O : a1 c : hi-A : o1 A : 1.0
#
O : a1 a : lo-A : o1 A : 1.0
O : a1 b : lo-A : o1 A : 1.0
O : a1 c : lo-A : o1 A : 1.0
#
O : a1 a : C : o1 C : 1.0
O : a1 b : C : o1 C : 1.0
O : a1 c : C : o1 C : 1.0
#
O : a1 a : D : o1 D : 1.0
O : a1 b : D : o1 D : 1.0
O : a1 c : D : o1 D : 1.0
#
O : a1 a : plus1 : o1 plus1 : 1.0
O : a1 b : plus1 : o1 plus1 : 1.0
O : a1 c : plus1 : o1 plus1 : 1.0
#
O : a1 a : minus1 : o1 minus1 : 1.0
O : a1 b : minus1 : o1 minus1 : 1.0
O : a1 c : minus1 : o1 minus1 : 1.0
#O: action1 listen : tiger-left : observation1 tiger-left : 0.85
#O: action1 listen : tiger-left : observation1 tiger-right : 0.15
#O: action1 listen : tiger-right : observation1 tiger-left : 0.15
#O: action1 listen : tiger-right : observation1 tiger-right : 0.85
#O: listen listen : tiger-right : hear-right hear-right : 0.7225
#O: listen listen : tiger-right : hear-left hear-right : 0.1275
#O: listen listen : tiger-right : hear-right hear-left : 0.1275
#O: listen listen : tiger-right : hear-left hear-left : 0.0225
#The rewards
#or
#    R: <a1 a2...an> : <start-state> : <end-state> :
#    %f %f ... %f
#or
#    R: <a1 a2...an> : <start-state> :
#    %f %f ... %f 
#    %f %f ... %f 
#    ...
#    %f %f ... %f
#
#Typical problems only use R(s,ja) which is specified by:
#   R: <a1 a2...an> : <start-state> : * : * : %f
R: * : plus1 : * : * : 2.0
R: * : minus1 : * : * : 0.0
#R: * *: * : * : * : -1
#R: action1 open-left : tiger-left : * : * : -100
#R: action1 open-right : tiger-right : * : * : -100
#R: action1 open-left : tiger-right : * : * : +10
#R: action1 open-right : tiger-left : * : * : 10
#R: open-left open-right: tiger-left : * : * : -100
#R: open-left open-right: tiger-right : * : * : -100
#R: open-right open-left: tiger-left : * : * : -100
#R: open-right open-left: tiger-right : * : * : -100
#R: open-left listen: tiger-left : * : * : -101
#R: listen open-right: tiger-right : * : * : -101
#R: listen open-left: tiger-left : * : * : -101
#R: open-right listen: tiger-right : * : * : -101
#R: listen open-right: tiger-left : * : * : 9
#R: listen open-left: tiger-right : * : * : 9
#R: open-right listen: tiger-left : * : * : 9
#R: open-left listen: tiger-right : * : * : 9

